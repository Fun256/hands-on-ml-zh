### 双曲正切函数 tanh (z) = 2σ(2z) – 1

>>>就像 Logistic 函数，它是 S 形的、连续的、可微的，但是它的输出值范围从-1到1（不是在 Logistic 函数的 0 到 1），
>>>这往往使每个层的输出在训练开始时或多或少都正则化了（即以 0 为中心）。这常常有助于加快收敛速度。

### Relu 函数
>>> ReLU (z) = max (0, z)。它是连续的，但不幸的是在z=0时不可微（斜率突然改变，这可以使梯度下降反弹）。然而，在实践中，它工作得很好，
>>> 并且具有快速计算的优点。最重要的是，它没有最大输出值的事实也有助于减少梯度下降期间的一些问题
