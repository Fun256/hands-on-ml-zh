支持向量机（SVM）是个非常强大并且有多种功能的机器学习模型，能够做线性或者非线性的分类，回归，甚至异常值检测。机器学习领域中最为流行的模型之一，
是任何学习机器学习的人必备的工具。SVM 特别适合应用于复杂但中小规模数据集的分类问题。

软间隔分类:

使用更加软性的模型。目的在保持“街道”尽可能大和避免间隔违规（例如：数据点出现在“街道”中央或者甚至在错误的一边）之间找到一个良好的平衡。

在 Scikit-Learn 库的 SVM 类，你可以用C超参数（惩罚系数）来控制这种平衡：较小的C会导致更宽的“街道”，但更多的间隔违规。
> C 较小，说明对间隔违规惩罚较少，“街道”会更宽

> C 越大，惩罚的越大，“街道”越小，限制越多，容易导致过拟合。

> 所以，使用SVM 过程中出现过拟合，可以减小C

hinge 损失函数：
> L(y) = max(0 , 1 – t⋅y)

其中，y是预测值(-1到1之间)，t为目标值(1或 -1)。其含义为，y的值在 -1到1之间即可，并不鼓励 |y|>1，即让某个样本能够正确分类就可以了，
不鼓励分类器过度自信，当样本与分割线的距离超过1时并不会有任何奖励。目的在于使分类器更专注于整体的分类误差。

通用的方法是用网格搜索（grid search 见第 2 章）去找到最优超参数。首先进行非常粗略的网格搜索一般会很快，然后在找到的最佳值进行更细的网格搜索。
对每个超参数的作用有一个很好的理解可以帮助你在正确的超参数空间找到合适的值。

高斯 RBF 核
> gamma 较大，判定边界变得不规则；gamma较小，判定边界会更平滑

> 所以，当模型过拟合，边界太不规则了，可以减小gamma，模型欠拟合时，可以增加gamma

> 一般来说，你应该先尝试线性核函数（记住LinearSVC比SVC(kernel="linear")要快得多），尤其是当训练集很大或者有大量的特征的情况下。
> 如果训练集不太大，你也可以尝试高斯径向基核（Gaussian RBF Kernel），它在大多数情况下都很有效。如果你有空闲的时间和计算能力，
> 你还可以使用交叉验证和网格搜索来试验其他的核函数，特别是有专门用于你的训练集数据结构的核函数。

使用 Scikit-Learn 的LinearSVR类去实现线性 SVM 回归


